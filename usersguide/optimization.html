

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optimization tips &mdash; PyTables 3.10.2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=6b5090df"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="filenode - simulating a filesystem with PyTables" href="filenode.html" />
    <link rel="prev" title="Filenode Module" href="libref/filenode_classes.html" />
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111342564-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-111342564-1');
</script>


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/logo-pytables-small.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">User’s Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cookbook/index.html">Cookbook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../other_material.html">Other Material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MIGRATING_TO_3.x.html">Migrating from 2.x to 3.x</a></li>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../project_pointers.html">Project pointers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development.html">Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev_team.html">Development Team</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PyTables</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">PyTables User’s Guide</a></li>
      <li class="breadcrumb-item active">Optimization tips</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/usersguide/optimization.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="optimization-tips">
<h1>Optimization tips<a class="headerlink" href="#optimization-tips" title="Link to this heading"></a></h1>
<blockquote class="epigraph">
<div><p>… durch planmässiges Tattonieren.</p>
<p>[… through systematic, palpable experimentation.]</p>
<p class="attribution">—Johann Karl Friedrich Gauss [asked how he came upon his theorems]</p>
</div></blockquote>
<p>On this chapter, you will get deeper knowledge of PyTables internals.
PyTables has many tunable features so that you can improve the performance of
your application.  If you are planning to deal with really large data, you
should read carefully this section in order to learn how to get an important
efficiency boost for your code.  But if your datasets are small (say, up to
10 MB) or your number of nodes is contained (up to 1000), you should not
worry about that as the default parameters in PyTables are already tuned for
those sizes (although you may want to adjust them further anyway).  At any
rate, reading this chapter will help you in your life with PyTables.</p>
<section id="understanding-chunking">
<h2>Understanding chunking<a class="headerlink" href="#understanding-chunking" title="Link to this heading"></a></h2>
<p>The underlying HDF5 library that is used by PyTables allows for certain
datasets (the so-called <em>chunked</em> datasets) to take the data in bunches of a
certain length, named <em>chunks</em>, and write them on disk as a whole, i.e. the
HDF5 library treats chunks as atomic objects and disk I/O is always made in
terms of complete chunks.  This allows data filters to be defined by the
application to perform tasks such as compression, encryption, check-summing,
etc. on entire chunks.</p>
<p>HDF5 keeps a B-tree in memory that is used to map chunk structures on disk.
The more chunks that are allocated for a dataset the larger the B-tree.
Large B-trees take memory and cause file storage overhead as well as more
disk I/O and higher contention for the metadata cache.  Consequently, it’s
important to balance between memory and I/O overhead (small B-trees) and time
to access data (big B-trees).</p>
<p>In the next couple of sections, you will discover how to inform PyTables
about the expected size of your datasets for allowing a sensible computation
of the chunk sizes.  Also, you will be presented some experiments so that you
can get a feeling on the consequences of manually specifying the chunk size.
Although doing this latter is only reserved to experienced people, these
benchmarks may allow you to understand more deeply the chunk size
implications and let you quickly start with the fine-tuning of this important
parameter.</p>
<section id="informing-pytables-about-expected-number-of-rows-in-tables-or-arrays">
<span id="expectedrowsoptim"></span><h3>Informing PyTables about expected number of rows in tables or arrays<a class="headerlink" href="#informing-pytables-about-expected-number-of-rows-in-tables-or-arrays" title="Link to this heading"></a></h3>
<p>PyTables can determine a sensible chunk size to your dataset size if you
help it by providing an estimation of the final number of rows for an
extensible leaf <a class="footnote-reference brackets" href="#id4" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.  You should provide this information at leaf creation
time by passing this value to the <code class="docutils literal notranslate"><span class="pre">expectedrows</span></code> argument of the
<a class="reference internal" href="libref/file_class.html#tables.File.create_table" title="tables.File.create_table"><code class="xref py py-meth docutils literal notranslate"><span class="pre">File.create_table()</span></code></a> method or <a class="reference internal" href="libref/file_class.html#tables.File.create_earray" title="tables.File.create_earray"><code class="xref py py-meth docutils literal notranslate"><span class="pre">File.create_earray()</span></code></a> method (see
<a class="reference internal" href="libref/homogenous_storage.html#earrayclassdescr"><span class="std std-ref">The EArray class</span></a>).</p>
<p>When your leaf size is bigger than 10 MB (take this figure only as a
reference, not strictly), by providing this guess you will be optimizing the
access to your data.  When the table or array size is larger than, say 100MB,
you are <em>strongly</em> suggested to provide such a guess; failing to do that may
cause your application to do very slow I/O operations and to demand <em>huge</em>
amounts of memory. You have been warned!</p>
</section>
<section id="fine-tuning-the-chunksize">
<span id="chunksizefinetune"></span><h3>Fine-tuning the chunksize<a class="headerlink" href="#fine-tuning-the-chunksize" title="Link to this heading"></a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This section is mostly meant for experts.  If you are a beginner, you
must know that setting manually the chunksize is a potentially dangerous
action.</p>
</div>
<p>Most of the time, informing PyTables about the extent of your dataset is
enough.  However, for more sophisticated applications, when one has special
requirements for doing the I/O or when dealing with really large datasets,
you should really understand the implications of the chunk size in order to
be able to find the best value for your own application.</p>
<p>You can specify the chunksize for every chunked dataset in PyTables by
passing the chunkshape argument to the corresponding constructors. It is
important to point out that chunkshape is not exactly the same thing than a
chunksize; in fact, the chunksize of a dataset can be computed multiplying
all the dimensions of the chunkshape among them and multiplying the outcome
by the size of the atom.</p>
<p>We are going to describe a series of experiments where an EArray of 15 GB is
written with different chunksizes, and then it is accessed in both sequential
(i.e. first element 0, then element 1 and so on and so forth until the data
is exhausted) and random mode (i.e. single elements are read randomly all
through the dataset). These benchmarks have been carried out with
PyTables 2.1 on a machine with an Intel Core2 processor &#64; 3 GHz and a RAID-0
made of two SATA disks spinning at 7200 RPM, and using GNU/Linux with an XFS
filesystem.  The script used for the benchmarks is available in
bench/optimal-chunksize.py.</p>
<p>In figures <a class="reference internal" href="#createtime-chunksize"><span class="std std-ref">Figure 1</span></a>,
<a class="reference internal" href="#filesizes-chunksize"><span class="std std-ref">Figure 2</span></a>, <a class="reference internal" href="#seqtime-chunksize"><span class="std std-ref">Figure 3</span></a>
and <a class="reference internal" href="#randomtime-chunksize"><span class="std std-ref">Figure 4</span></a>, you can see how the chunksize
affects different aspects, like creation time, file sizes, sequential read
time and random read time.  So, if you properly inform PyTables about the
extent of your datasets, you will get an automatic chunksize value (256 KB in
this case) that is pretty optimal for most of uses.  However, if what you
want is, for example, optimize the creation time when using the
Zlib compressor, you may want to reduce the chunksize to 32 KB (see
<a class="reference internal" href="#createtime-chunksize"><span class="std std-ref">Figure 1</span></a>). Or, if your goal is to optimize the
sequential access time for an dataset compressed with Blosc, you may want to
increase the chunksize to 512 KB (see <a class="reference internal" href="#seqtime-chunksize"><span class="std std-ref">Figure 3</span></a>).</p>
<p>You will notice that, by manually specifying the chunksize of a leave you
will not normally get a drastic increase in performance, but at least, you
have the opportunity to fine-tune such an important parameter for improve
performance.</p>
<figure class="align-center" id="id7">
<span id="createtime-chunksize"></span><img alt="../_images/create-chunksize-15GB.png" src="../_images/create-chunksize-15GB.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 1. Creation time per element for a 15 GB EArray and different
chunksizes.</strong></span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id8">
<span id="filesizes-chunksize"></span><img alt="../_images/filesizes-chunksize-15GB.png" src="../_images/filesizes-chunksize-15GB.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 2. File sizes for a 15 GB EArray and different chunksizes.</strong></span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id9">
<span id="seqtime-chunksize"></span><img alt="../_images/seq-chunksize-15GB.png" src="../_images/seq-chunksize-15GB.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 3. Sequential access time per element for a 15 GB EArray and
different chunksizes.</strong></span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id10">
<span id="randomtime-chunksize"></span><img alt="../_images/random-chunksize-15GB.png" src="../_images/random-chunksize-15GB.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 4. Random access time per element for a 15 GB EArray and
different chunksizes.</strong></span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Finally, it is worth noting that adjusting the chunksize can be specially
important if you want to access your dataset by blocks of certain dimensions.
In this case, it is normally a good idea to set your chunkshape to be the
same than these dimensions; you only have to be careful to not end with a too
small or too large chunksize.  As always, experimenting prior to pass your
application into production is your best ally.</p>
</section>
<section id="multidimensional-slicing-and-chunk-block-sizes">
<span id="multidimslicing"></span><h3>Multidimensional slicing and chunk/block sizes<a class="headerlink" href="#multidimensional-slicing-and-chunk-block-sizes" title="Link to this heading"></a></h3>
<p>When working with multidimensional arrays, you may find yourself accessing
array slices smaller than chunks.  In that case, reducing the chunksize to
match your slices may result in too many chunks and large B-trees in memory,
but avoiding that by using bigger chunks may make operations slow, as whole
big chunks would need to be loaded in memory anyway just to access a small
slice.  Compression may help by making the chunks smaller on disk, but the
decompressed chunks in memory may still be big enough to make the slicing
inefficient.</p>
<p>To help with this scenario, PyTables supports Blosc2 NDim (b2nd) compression,
which uses a 2-level partitioning of arrays into multidimensional chunks made
of smaller multidimensional blocks; reading slices of such arrays is optimized
by direct access to chunks (avoiding the slower HDF5 filter pipeline).  b2nd
works better when the compressed chunk is big enough to minimize the number of
chunks in the array, while still fitting in the CPU’s L3 cache (and leaving
some extra room), and the blocksize is such that any block is likely to fit
both compressed and uncompressed in each CPU core’s L2 cache.</p>
<p>This last requirement of course depends on the data itself and the compression
algorithm and parameters chosen.  The Blosc2 library uses some heuristics to
choose a good enough blocksize given a compression algorithm and level.  Once
you choose an algorithm which fits your data, you may use the program
examples/get_blocksize.c in the C-Blosc2 package to get the default blocksize
for the different levels of that compression algorithm (it already contains
some sample output for zstd).  In the example benchmarks below, with the LZ4
algorithm with shuffling and compression level 8, we got a blocksize of 2MB
which fitted in the L2 cache of each of our CPU cores.</p>
<p>With that compression setup, we ran the benchmark in
bench/b2nd_compare_getslice.py, which compares the throughput of slicing a
4-dimensional array of 50x100x300x250 long floats (2.8GB) along each of its
dimensions for PyTables with flat slicing (via the HDF5 filter mechanism),
PyTables with b2nd slicing (optimized via direct chunk access), and h5py
(which also uses the HDF5 filter).  We repeated the benchmark with different
values of the <code class="docutils literal notranslate"><span class="pre">BLOSC_NTHREADS</span></code> environment variable so as to find the best
number of parallel Blosc2 threads (6 for our CPU).  The result for the
original chunkshape of 10x25x50x50 is shown in <a class="reference internal" href="#b2ndslicing-vs-filter-smallchunk"><span class="std std-ref">figure</span></a>.</p>
<figure class="align-center" id="id11">
<span id="b2ndslicing-vs-filter-smallchunk"></span><img alt="../_images/b2nd_getslice_small.png" src="../_images/b2nd_getslice_small.png" />
<figcaption>
<p><span class="caption-text"><strong>Throughput for slicing a Blosc2-compressed array along its 4 dimensions
with PyTables filter, PyTables b2nd (optimized) and h5py, all using a
small chunk.</strong></span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The optimized b2nd slicing of PyTables already provided some sizable speedups
in comparison with flat slicing based on the HDF5 filter pipeline in the inner
dimensions.  But the 4.8MB chunksize was probably too small for the CPU’s L3
cache of 36MB, causing extra overhead.  By proceeding to adjust the chunkshape
to 10x25x150x100 (28.6MB), that fits in the L3 cache, the performance gains
are palpable, as shown in <a class="reference internal" href="#b2ndslicing-vs-filter-bigchunk"><span class="std std-ref">figure</span></a>.</p>
<figure class="align-center" id="id12">
<span id="b2ndslicing-vs-filter-bigchunk"></span><img alt="../_images/b2nd_getslice_big.png" src="../_images/b2nd_getslice_big.png" />
<figcaption>
<p><span class="caption-text"><strong>Throughput for slicing a Blosc2-compressed array along its 4 dimensions
with PyTables filter, PyTables b2nd (optimized) and h5py, all using a big
chunk.</strong></span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Choosing a better chunkshape not just provided up to 5x speedup for the
PyTables optimized case, it also resulted in 3x-4x speedups compared to the
performance of the HDF5 filter.</p>
<p>Optimized b2nd slicing in PyTables has its limitations, though: it only works
with contiguous slices (that is, with step 1 on every dimension) and on
datasets with the same byte ordering as the host machine and where Blosc2 is
the only filter in the HDF5 pipeline (e.g. Fletcher32 checksumming is off).
However, the performance gains may be worth the extra effort of choosing the
right compression parameters and chunksizes when your use case is not affected
by those limitations.</p>
</section>
<section id="low-level-access-to-chunks-direct-chunking">
<span id="directchunking"></span><h3>Low-level access to chunks: direct chunking<a class="headerlink" href="#low-level-access-to-chunks-direct-chunking" title="Link to this heading"></a></h3>
<p>Features like the aforementioned optimized b2nd slicing may spare you from
some of the overhead of the HDF5 filter pipeline.  However, you may still want
finer control on the processing of chunk data beyond what is offered natively
by PyTables.</p>
<p>The direct chunking API allows you to query, read and write raw chunks
completely bypassing the HDF5 filter pipeline.  As we will see <a class="reference internal" href="#comprdirectchunking"><span class="std std-ref">while
discussing compression</span></a>, this may allow you to better
customize the store/load process and get substantial performance increases.
You may also use a compressor not supported at all by PyTables or HDF5, for
instance to help you develop an HDF5 C plugin for it, by first writing chunks
in Python and implementing decompression in C for transparent reading
operations.</p>
<p>As an example, direct chunking support has been used in h5py to create
JPEG2000-compressed tomographies which may be accessed normally on any device
with h5py, hdf5plugin and the blosc2_grok plugin installed.  Creating such
datasets using normal array operations would have otherwise been impossible in
PyTables or h5py.</p>
<p>Be warned that this is a very low-level functionality.  If done right
(e.g. using proper in-memory layout and a robust compressor), you may be able
to very efficiently produce datasets compatible with other HDF5 libraries, but
you may otherwise produce broken or incompatible datasets.  As usual, consider
your requirements and take measurements.</p>
<p>The direct chunking API relies on three operations supported by any chunked
leaf (CArray, EArray, Table).  The first one is <a class="reference internal" href="libref/hierarchy_classes.html#tables.Leaf.chunk_info" title="tables.Leaf.chunk_info"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Leaf.chunk_info()</span></code></a>, which
returns a <a class="reference internal" href="libref/helper_classes.html#chunkinfoclassdescr"><span class="std std-ref">ChunkInfo</span></a> instance with information
about the chunk containing the item at the given coordinates:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">carray</span> <span class="o">=</span> <span class="n">h5f</span><span class="o">.</span><span class="n">create_carray</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="s1">&#39;carray&#39;</span><span class="p">,</span> <span class="n">chunkshape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,),</span> <span class="n">obj</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">carray</span><span class="o">.</span><span class="n">chunk_info</span><span class="p">((</span><span class="mi">42</span><span class="p">,))</span>
<span class="go">ChunkInfo(start=(40,), filter_mask=0, offset=4040, size=80)</span>
</pre></div>
</div>
<p>This means that the item at coordinate 42 is stored in a chunk of 80 bytes
which starts at coordinate 40 in the array and byte 4040 in the file.  The
latter offset may be used to let other code access the chunk directly on
storage (optimized b2nd slicing uses that approach).  The former chunk start
coordinate may come in handy for other chunk operations, which expect chunk
start coordinates.</p>
<p>The second operation is <a class="reference internal" href="libref/hierarchy_classes.html#tables.Leaf.write_chunk" title="tables.Leaf.write_chunk"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Leaf.write_chunk()</span></code></a>, which stores the raw data
bytes for the chunk that starts at the given coordinates.  The data must
already have gone through dataset filters (i.e. compression):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This is only to signal others that Blosc2 compression is used,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># as actual parameters are stored in the chunk itself.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b2filters</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">Filters</span><span class="p">(</span><span class="n">complevel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">complib</span><span class="o">=</span><span class="s1">&#39;blosc2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">earray</span> <span class="o">=</span> <span class="n">h5f</span><span class="o">.</span><span class="n">create_earray</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="s1">&#39;earray&#39;</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">b2filters</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">chunkshape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,),</span> <span class="n">obj</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cdata</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">210</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cdata_b2</span> <span class="o">=</span> <span class="n">blosc2</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">cdata</span><span class="p">)</span>  <span class="c1"># compress</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">chunk</span> <span class="o">=</span> <span class="n">cdata_b2</span><span class="o">.</span><span class="n">to_cframe</span><span class="p">()</span>  <span class="c1"># serialize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">earray</span><span class="o">.</span><span class="n">truncate</span><span class="p">((</span><span class="mi">110</span><span class="p">,))</span>  <span class="c1"># enlarge array cheaply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">earray</span><span class="o">.</span><span class="n">write_chunk</span><span class="p">((</span><span class="mi">100</span><span class="p">,),</span> <span class="n">chunk</span><span class="p">)</span>  <span class="c1"># write directly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">earray</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span> <span class="o">==</span> <span class="n">cdata</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>  <span class="c1"># access new chunk as usual</span>
<span class="go">True</span>
</pre></div>
</div>
<p>The third operation is <a class="reference internal" href="libref/hierarchy_classes.html#tables.Leaf.read_chunk" title="tables.Leaf.read_chunk"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Leaf.read_chunk()</span></code></a>, which loads the raw data bytes
for the chunk that starts at the given coordinates.  The data needs to go
through dataset filters (i.e. decompression):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cdata</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">210</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rchunk</span> <span class="o">=</span> <span class="n">earray</span><span class="o">.</span><span class="n">read_chunk</span><span class="p">((</span><span class="mi">100</span><span class="p">,))</span>  <span class="c1"># read directly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rcdata_b2</span> <span class="o">=</span> <span class="n">blosc2</span><span class="o">.</span><span class="n">ndarray_from_cframe</span><span class="p">(</span><span class="n">rchunk</span><span class="p">)</span>  <span class="c1"># deserialize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rcdata</span> <span class="o">=</span> <span class="n">rcdata_b2</span><span class="p">[:]</span>  <span class="c1"># decompress</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">rcdata</span> <span class="o">==</span> <span class="n">cdata</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="go">True</span>
</pre></div>
</div>
<p>The file examples/direct-chunking.py contains a more elaborate illustration of direct chunking.</p>
</section>
</section>
<section id="accelerating-your-searches">
<span id="searchoptim"></span><h2>Accelerating your searches<a class="headerlink" href="#accelerating-your-searches" title="Link to this heading"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Many of the explanations and plots in this section and the forthcoming
ones still need to be updated to include Blosc (see
<a class="reference internal" href="bibliography.html#blosc"><span class="std std-ref">[BLOSC]</span></a>) or Blosc2 (see <a class="reference internal" href="bibliography.html#blosc2"><span class="std std-ref">[BLOSC2]</span></a>),
the new and powerful compressors added in
PyTables 2.2 and PyTables 3.8 respectively.  You should expect them to be
the fastest compressors
among all the described here, and their use is strongly recommended
whenever you need extreme speed while keeping good compression ratios.
However, below you can still find some sections describing the advantages
of using Blosc/Blosc2 in PyTables.</p>
</div>
<p>Searching in tables is one of the most common and time consuming operations
that a typical user faces in the process of mining through his data.  Being
able to perform queries as fast as possible will allow more opportunities for
finding the desired information quicker and also allows to deal with larger
datasets.</p>
<p>PyTables offers many sort of techniques so as to speed-up the search process
as much as possible and, in order to give you hints to use them based, a
series of benchmarks have been designed and carried out.  All the results
presented in this section have been obtained with synthetic, random data and
using PyTables 2.1.  Also, the tests have been conducted on a machine with an
Intel Core2 (64-bit) &#64; 3 GHz processor with RAID-0 disk storage (made of four
spinning disks &#64; 7200 RPM), using GNU/Linux with an XFS filesystem.  The
script used for the benchmarks is available in bench/indexed_search.py.
As your data, queries and platform may be totally different for your case,
take this just as a guide because your mileage may vary (and will vary).</p>
<p>In order to be able to play with tables with a number of rows as large as
possible, the record size has been chosen to be rather small (24 bytes). Here
it is its definition:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Record</span><span class="p">(</span><span class="n">tables</span><span class="o">.</span><span class="n">IsDescription</span><span class="p">):</span>
    <span class="n">col1</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">Int32Col</span><span class="p">()</span>
    <span class="n">col2</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">Int32Col</span><span class="p">()</span>
    <span class="n">col3</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">Float64Col</span><span class="p">()</span>
    <span class="n">col4</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">Float64Col</span><span class="p">()</span>
</pre></div>
</div>
<p>In the next sections, we will be optimizing the times for a relatively
complex query like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">table</span> <span class="k">if</span> <span class="p">(</span>
          <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;col4&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">lim1</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;col4&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lim2</span><span class="p">)</span> <span class="ow">or</span>
          <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">lim3</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lim4</span><span class="p">]))</span> <span class="ow">and</span>
          <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;col1&#39;</span><span class="p">]</span><span class="o">+</span><span class="mf">3.1</span><span class="o">*</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;col3&#39;</span><span class="p">]</span><span class="o">*</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;col4&#39;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">lim5</span><span class="p">)</span>
          <span class="p">)]</span>
</pre></div>
</div>
<p>(for future reference, we will call this sort of queries <em>regular</em> queries).
So, if you want to see how to greatly improve the time taken to run queries
like this, keep reading.</p>
<section id="in-kernel-searches">
<span id="inkernelsearch"></span><h3>In-kernel searches<a class="headerlink" href="#in-kernel-searches" title="Link to this heading"></a></h3>
<p>PyTables provides a way to accelerate data selections inside of a single
table, through the use of the <a class="reference internal" href="libref/structured_storage.html#tablemethods-querying"><span class="std std-ref">Table methods - querying</span></a> iterator and
related query methods. This mode of selecting data is called <em>in-kernel</em>.
Let’s see an example of an <em>in-kernel</em> query based on the <em>regular</em> one
mentioned above:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&#39;&#39;&#39;(((col4 &gt;= lim1) &amp; (col4 &lt; lim2)) |</span>
<span class="sd">               ((col2 &gt; lim3) &amp; (col2 &lt; lim4)) &amp;</span>
<span class="sd">               ((col1+3.1*col2+col3*col4) &gt; lim5))&#39;&#39;&#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>This simple change of mode selection can improve search times quite a lot and
actually make PyTables very competitive when compared against typical
relational databases as you can see in <a class="reference internal" href="#sequentialtimes-10m"><span class="std std-ref">Figure 5</span></a>
and <a class="reference internal" href="#sequentialtimes-1g"><span class="std std-ref">Figure 6</span></a>.</p>
<figure class="align-center" id="id13">
<span id="sequentialtimes-10m"></span><img alt="../_images/Q7-10m-noidx.png" src="../_images/Q7-10m-noidx.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 5. Times for non-indexed complex queries in a small table with
10 millions of rows: the data fits in memory.</strong></span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>By looking at <a class="reference internal" href="#sequentialtimes-10m"><span class="std std-ref">Figure 5</span></a> you can see how in the
case that table data fits easily in memory, in-kernel searches on
uncompressed tables are generally much faster (10x) than standard queries as
well as PostgreSQL (5x).  Regarding compression, we can see how Zlib
compressor actually slows down the performance of in-kernel queries by a
factor 3.5x; however, it remains faster than PostgreSQL (40%).
On his hand, LZO compressor only decreases the performance by a 75% with
respect to uncompressed in-kernel queries and is still a lot faster than
PostgreSQL (3x).  Finally, one can observe that, for low selectivity queries
(large number of hits), PostgreSQL performance degrades quite steadily, while
in PyTables this slow down rate is significantly smaller.  The reason of this
behaviour is not entirely clear to the authors, but the fact is clearly
reproducible in our benchmarks.</p>
<p>But, why in-kernel queries are so fast when compared with regular ones?.
The answer is that in regular selection mode the data for all the rows in
table has to be brought into Python space so as to evaluate the condition and
decide if the corresponding field should be added to the result list.  On the
contrary, in the in-kernel mode, the condition is passed to the PyTables
kernel (hence the name), written in C, and evaluated there at full C speed
(with the help of the integrated Numexpr package, see
<a class="reference internal" href="bibliography.html#numexpr"><span class="std std-ref">[NUMEXPR]</span></a>), so that the only values that are brought to
Python space are the rows that fulfilled the condition.  Hence, for
selections that only have a relatively small number of hits (compared with
the total amount of rows), the savings are very large.  It is also
interesting to note the fact that, although for queries with a large number
of hits the speed-up is not as high, it is still very important.</p>
<p>On the other hand, when the table is too large to fit in memory (see
<a class="reference internal" href="#sequentialtimes-1g"><span class="std std-ref">Figure 6</span></a>), the difference in speed between
regular and in-kernel is not so important, but still significant (2x).  Also,
and curiously enough, large tables compressed with Zlib offers slightly
better performance (around 20%) than uncompressed ones; this is because the
additional CPU spent by the uncompressor is compensated by the savings in
terms of net I/O (one has to read less actual data from disk).  However, when
using the extremely fast LZO compressor, it gives a clear advantage over
Zlib, and is up to 2.5x faster than not using compression at all.  The reason
is that LZO decompression speed is much faster than Zlib, and that allows
PyTables to read the data at full disk speed (i.e. the bottleneck is in the
I/O subsystem, not in the CPU).  In this case the compression rate is around
2.5x, and this is why the data can be read 2.5x faster.  So, in general,
using the LZO compressor is the best way to ensure best reading/querying
performance for out-of-core datasets (more about how compression affects
performance in <a class="reference internal" href="#compressionissues"><span class="std std-ref">Compression issues</span></a>).</p>
<figure class="align-center" id="id14">
<span id="sequentialtimes-1g"></span><img alt="../_images/Q8-1g-noidx.png" src="../_images/Q8-1g-noidx.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 6. Times for non-indexed complex queries in a large table with 1
billion of rows: the data does not fit in memory.</strong></span><a class="headerlink" href="#id14" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Furthermore, you can mix the <em>in-kernel</em> and <em>regular</em> selection modes for
evaluating arbitrarily complex conditions making use of external functions.
Look at this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="p">[</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;var2&#39;</span><span class="p">]</span>
           <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">&#39;(var3 == &quot;foo&quot;) &amp; (var1 &lt;= 20)&#39;</span><span class="p">)</span>
           <span class="k">if</span> <span class="n">your_function</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;var2&#39;</span><span class="p">])</span> <span class="p">]</span>
</pre></div>
</div>
<p>Here, we use an <em>in-kernel</em> selection to choose rows according to the values
of the var3 and var1 fields.  Then, we apply a <em>regular</em> selection to
complete the query. Of course, when you mix the <em>in-kernel</em> and <em>regular</em>
selection modes you should pass the most restrictive condition to the
<em>in-kernel</em> part, i.e. to the where() iterator.  In situations where it is
not clear which is the most restrictive condition, you might want to
experiment a bit in order to find the best combination.</p>
<p>However, since in-kernel condition strings allow rich expressions allowing
the coexistence of multiple columns, variables, arithmetic operations and
many typical functions, it is unlikely that you will be forced to use
external regular selections in conditions of small to medium complexity.
See <a class="reference internal" href="condition_syntax.html#condition-syntax"><span class="std std-ref">Condition Syntax</span></a> for more information on in-kernel condition
syntax.</p>
<p>As mentioned before Blosc and Blosc2 can bring a lot of acceleration to your
queries.  Blosc2 is the new generation of the Blosc compressor, and PyTables
uses it in a way that complements the existing Blosc HDF5 filter.
Just as comparison point, below there is a profile of 6 different
in-kernel queries using the standard Zlib, Blosc and Blosc2 compressors:</p>
<figure class="align-center" id="id15">
<span id="inkernelperformance-zlib-blosc-blosc2"></span><img alt="../_images/inkernel-zlib-blosc-blosc2.png" src="../_images/inkernel-zlib-blosc-blosc2.png" />
<figcaption>
<p><span class="caption-text"><strong>Times for 6 complex queries in a large table with 100 million rows.
Data comes from an actual set of meteorological measurements.</strong></span><a class="headerlink" href="#id15" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>As you can see, Blosc, but specially Blosc2, can get much better performance
than the Zlib compressor when doing complex queries.  Blosc can be up to
6x faster, whereas Blosc2 can be more than 13x times faster (i.e. processing
data at more than 8 GB/s).  Perhaps more interestingly, Blosc2 can make
inkernel queries <em>faster</em> than using uncompressed data; and although it might
seem that we are dealing with on-disk data, it is actually in memory because,
after the first query, all the data has been cached in memory by the
OS filesystem cache.  That also means that for actual data that is on-disk,
the advantages of using Blosc/Blosc2 can be much more than this.</p>
<p>In case you might want to compress as much as possible, Blosc and Blosc2 allow
to use different compression codecs underneath.  For example, in the same
<a class="reference internal" href="#inkernelperformance-zlib-blosc-blosc2"><span class="std std-ref">figure</span></a> that we are discussing,
one can see another line where Blosc2 is used in combination with
Zstd, and this combination is providing a compression ratio of 9x; this is
actually larger than the one achieved by the Zlib compressor embedded in HDF5,
which is 7.6x –BTW, the compression ratio of Blosc2 using the default compressor
is 7.4x, not that far from standalone Zlib.</p>
<p>In this case, the Blosc2 + Zstd combination still makes inkernel queries
faster than with no using compression; so with Blosc2 you can have the best of the
two worlds: top-class speed while keeping good compression ratios (see
<a class="reference internal" href="#compressionratio-zlib-blosc-blosc2"><span class="std std-ref">figure</span></a>).</p>
<figure class="align-center" id="id16">
<span id="compressionratio-zlib-blosc-blosc2"></span><img alt="../_images/cratio-zlib-blosc-blosc2.png" src="../_images/cratio-zlib-blosc-blosc2.png" />
<figcaption>
<p><span class="caption-text"><strong>Compression ratio for different codecs and the 100 million rows table.
Data comes from an actual set of meteorological measurements.</strong></span><a class="headerlink" href="#id16" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Furthermore, and despite that the dataset is 3.1 GB in size, the memory
consumption after the 6 queries is still less than 250 MB.
That means that you can do queries of large, on-disk datasets with machines
with much less RAM than the dataset and still get all the speed that the disk
can provide.</p>
<p>But that is not all, provided that Blosc2 can be faster than memory, it
can accelerate memory access too.  For example, in
<a class="reference internal" href="#inkernelperformance-vs-pandas"><span class="std std-ref">figure</span></a> we can see that, when the
HDF5 file is in the operating system cache, inkernel queries using Blosc2
can be very close in performance even when compared with pandas.  That
is because the boost provided by Blosc2 compression almost compensate the
overhead of HDF5 and the filesystem layers.</p>
<figure class="align-center" id="id17">
<span id="inkernelperformance-vs-pandas"></span><img alt="../_images/inkernel-vs-pandas.png" src="../_images/inkernel-vs-pandas.png" />
<figcaption>
<p><span class="caption-text"><strong>Performance for 6 complex queries in a large table with 100 million rows.
Both PyTables and pandas use the numexpr engine behind the scenes.</strong></span><a class="headerlink" href="#id17" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>You can see more info about Blosc2 and how it collaborates with HDF5 for
achieving such a high I/O speed in our blog at:
<a class="reference external" href="https://www.blosc.org/posts/blosc2-pytables-perf/">https://www.blosc.org/posts/blosc2-pytables-perf/</a></p>
</section>
<section id="indexed-searches">
<h3>Indexed searches<a class="headerlink" href="#indexed-searches" title="Link to this heading"></a></h3>
<p>When you need more speed than <em>in-kernel</em> selections can offer you, PyTables
offers a third selection method, the so-called <em>indexed</em> mode (based on the
highly efficient OPSI indexing engine ).  In this mode, you have to decide
which column(s) you are going to apply your selections over, and index them.
Indexing is just a kind of sorting operation over a column, so that searches
along such a column (or columns) will look at this sorted information by
using a <em>binary search</em> which is much faster than the <em>sequential search</em>
described in the previous section.</p>
<p>You can index the columns you want by calling the <a class="reference internal" href="libref/structured_storage.html#tables.Column.create_index" title="tables.Column.create_index"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Column.create_index()</span></code></a>
method on an already created table.  For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">indexrows</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">cols</span><span class="o">.</span><span class="n">var1</span><span class="o">.</span><span class="n">create_index</span><span class="p">()</span>
<span class="n">indexrows</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">cols</span><span class="o">.</span><span class="n">var2</span><span class="o">.</span><span class="n">create_index</span><span class="p">()</span>
<span class="n">indexrows</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">cols</span><span class="o">.</span><span class="n">var3</span><span class="o">.</span><span class="n">create_index</span><span class="p">()</span>
</pre></div>
</div>
<p>will create indexes for all var1, var2 and var3 columns.</p>
<p>After you have indexed a series of columns, the PyTables query optimizer will
try hard to discover the usable indexes in a potentially complex expression.
However, there are still places where it cannot determine that an index can
be used. See below for examples where the optimizer can safely determine if
an index, or series of indexes, can be used or not.</p>
<p>Example conditions where an index can be used:</p>
<ul class="simple">
<li><p>var1 &gt;= “foo” (var1 is used)</p></li>
<li><p>var1 &gt;= mystr (var1 is used)</p></li>
<li><p>(var1 &gt;= “foo”) &amp; (var4 &gt; 0.0) (var1 is used)</p></li>
<li><p>(“bar” &lt;= var1) &amp; (var1 &lt; “foo”) (var1 is used)</p></li>
<li><p>((“bar” &lt;= var1) &amp; (var1 &lt; “foo”)) &amp; (var4 &gt; 0.0) (var1 is used)</p></li>
<li><p>(var1 &gt;= “foo”) &amp; (var3 &gt; 10) (var1 and var3 are used)</p></li>
<li><p>(var1 &gt;= “foo”) | (var3 &gt; 10) (var1 and var3 are used)</p></li>
<li><p>~(var1 &gt;= “foo”) | ~(var3 &gt; 10) (var1 and var3 are used)</p></li>
</ul>
<p>Example conditions where an index can <em>not</em> be used:</p>
<ul class="simple">
<li><p>var4 &gt; 0.0 (var4 is not indexed)</p></li>
<li><p>var1 != 0.0 (range has two pieces)</p></li>
<li><p>~((“bar” &lt;= var1) &amp; (var1 &lt; “foo”)) &amp; (var4 &gt; 0.0) (negation of a complex boolean expression)</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>From PyTables 2.3 on, several indexes can be used in a single query.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to know for sure whether a particular query will use indexing
or not (without actually running it), you are advised to use the
<a class="reference internal" href="libref/structured_storage.html#tables.Table.will_query_use_indexing" title="tables.Table.will_query_use_indexing"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Table.will_query_use_indexing()</span></code></a> method.</p>
</div>
<p>One important aspect of the new indexing in PyTables (&gt;= 2.3) is that it has
been designed from the ground up with the goal of being capable to
effectively manage very large tables.  To this goal, it sports a wide
spectrum of different quality levels (also called optimization levels) for
its indexes so that the user can choose the best one that suits her needs
(more or less size, more or less performance).</p>
<p>In <a class="reference internal" href="#createindextimes"><span class="std std-ref">Figure 7</span></a>, you can see that the times to index
columns in tables can be really short.  In particular, the time to index a
column with 1 billion rows (1 Gigarow) with the lowest optimization level is
less than 4 minutes while indexing the same column with full optimization (so
as to get a completely sorted index or CSI) requires around 1 hour.  These
are rather competitive figures compared with a relational database (in this
case, PostgreSQL 8.3.1, which takes around 1.5 hours for getting the index
done).  This is because PyTables is geared towards read-only or append-only
tables and takes advantage of this fact to optimize the indexes properly.  On
the contrary, most relational databases have to deliver decent performance in
other scenarios as well (specially updates and deletions), and this fact
leads not only to slower index creation times, but also to indexes taking
much more space on disk, as you can see in <a class="reference internal" href="#indexsizes"><span class="std std-ref">Figure 8</span></a>.</p>
<figure class="align-center" id="id18">
<span id="createindextimes"></span><img alt="../_images/create-index-time-int32-float64.png" src="../_images/create-index-time-int32-float64.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 7. Times for indexing an Int32 and Float64 column.</strong></span><a class="headerlink" href="#id18" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id19">
<span id="indexsizes"></span><img alt="../_images/indexes-sizes2.png" src="../_images/indexes-sizes2.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 8. Sizes for an index of a Float64 column with 1 billion of rows.</strong></span><a class="headerlink" href="#id19" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The user can select the index quality by passing the desired optlevel and
kind arguments to the <a class="reference internal" href="libref/structured_storage.html#tables.Column.create_index" title="tables.Column.create_index"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Column.create_index()</span></code></a> method.  We can see in
figures <a class="reference internal" href="#createindextimes"><span class="std std-ref">Figure 7</span></a> and <a class="reference internal" href="#indexsizes"><span class="std std-ref">Figure 8</span></a>
how the different optimization levels affects index time creation and index
sizes.</p>
<p>So, which is the effect of the different optimization levels in terms of
query times?  You can see that in <a class="reference internal" href="#querytimes-indexed-optlevels"><span class="std std-ref">Figure 9</span></a>.</p>
<figure class="align-center" id="id20">
<span id="querytimes-indexed-optlevels"></span><img alt="../_images/Q8-1g-idx-optlevels.png" src="../_images/Q8-1g-idx-optlevels.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 9. Times for complex queries with a cold cache (mean of 5 first
random queries) for different optimization levels. Benchmark made on a machine with Intel Core2 (64-bit) &#64; 3 GHz processor with RAID-0 disk storage.</strong></span><a class="headerlink" href="#id20" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Of course, compression also has an effect when doing indexed queries,
although not very noticeable, as can be seen in
<a class="reference internal" href="#querytimes-indexed-compress"><span class="std std-ref">Figure 10</span></a>.
As you can see, the difference between using no compression and using Zlib or
LZO is very little, although LZO achieves relatively better performance
generally speaking.</p>
<figure class="align-center" id="id21">
<span id="querytimes-indexed-compress"></span><img alt="../_images/Q8-1g-idx-compress.png" src="../_images/Q8-1g-idx-compress.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 10. Times for complex queries with a cold cache (mean of 5 first
random queries) for different compressors.</strong></span><a class="headerlink" href="#id21" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>You can find a more complete description and benchmarks about OPSI, the
indexing system of PyTables (&gt;= 2.3) in <a class="reference internal" href="bibliography.html#opsi"><span class="std std-ref">[OPSI]</span></a>.</p>
</section>
<section id="indexing-and-solid-state-disks-ssd">
<h3>Indexing and Solid State Disks (SSD)<a class="headerlink" href="#indexing-and-solid-state-disks-ssd" title="Link to this heading"></a></h3>
<p>Lately, the long promised Solid State Disks (SSD for brevity) with decent
capacities and affordable prices have finally hit the market and will
probably stay in coexistence with the traditional spinning disks for the
foreseeable future (separately or forming <em>hybrid</em> systems).  SSD have many
advantages over spinning disks, like much less power consumption and better
throughput.  But of paramount importance, specially in the context of
accelerating indexed queries, is its very reduced latency during disk seeks,
which is typically 100x better than traditional disks.
Such a huge improvement has to have a clear impact in reducing the query
times, specially when the selectivity is high (i.e. the number of hits is
small).</p>
<p>In order to offer an estimate on the performance improvement we can expect
when using a low-latency SSD instead of traditional spinning disks, the
benchmark in the previous section has been repeated, but this time using a
single SSD disk instead of the four spinning disks in RAID-0.  The result can
be seen in <a class="reference internal" href="#querytimes-indexed-ssd"><span class="std std-ref">Figure 11</span></a>.  There one can see how
a query in a table of 1 billion of rows with 100 hits took just 1 tenth of
second when using a SSD, instead of 1 second that needed the RAID made of
spinning disks.  This factor of 10x of speed-up for high-selectivity queries
is nothing to sneeze at, and should be kept in mind when really high
performance in queries is needed.  It is also interesting that using
compression with LZO does have a clear advantage over when no compression is
done.</p>
<figure class="align-center" id="id22">
<span id="querytimes-indexed-ssd"></span><img alt="../_images/Q8-1g-idx-SSD.png" src="../_images/Q8-1g-idx-SSD.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 11. Times for complex queries with a cold cache (mean of 5 first
random queries) for different disk storage (SSD vs spinning disks).</strong></span><a class="headerlink" href="#id22" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Finally, we should remark that SSD can’t compete with traditional spinning
disks in terms of capacity as they can only provide, for a similar cost,
between 1/10th and 1/50th of the size of traditional disks.  It is here where
the compression capabilities of PyTables can be very helpful because both
tables and indexes can be compressed and the final space can be reduced by
typically 2x to 5x (4x to 10x when compared with traditional relational
databases).
Best of all, as already mentioned, performance is not degraded when
compression is used, but actually <em>improved</em>.
So, by using PyTables and SSD you can query larger datasets that otherwise
would require spinning disks when using other databases</p>
<p>In fact, we were unable to run the PostgreSQL benchmark in this case because
the space needed exceeded the capacity of our SSD., while allowing
improvements in the speed of indexed queries between 2x (for medium to low
selectivity queries) and 10x (for high selectivity queries).</p>
</section>
<section id="achieving-ultimate-speed-sorted-tables-and-beyond">
<h3>Achieving ultimate speed: sorted tables and beyond<a class="headerlink" href="#achieving-ultimate-speed-sorted-tables-and-beyond" title="Link to this heading"></a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Sorting a large table is a costly operation.  The next procedure should
only be performed when your dataset is mainly read-only and meant to be
queried many times.</p>
</div>
<p>When querying large tables, most of the query time is spent in locating the
interesting rows to be read from disk.  In some occasions, you may have
queries whose result depends <em>mainly</em> of one single column (a query with only
one single condition is the trivial example), so we can guess that sorting
the table by this column would lead to locate the interesting rows in a much
more efficient way (because they would be mostly <em>contiguous</em>).  We are going
to confirm this guess.</p>
<p>For the case of the query that we have been using in the previous sections:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;col2&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
<span class="w">            </span><span class="sd">&#39;&#39;&#39;(((col4 &gt;= lim1) &amp; (col4 &lt; lim2)) |</span>
<span class="sd">               ((col2 &gt; lim3) &amp; (col2 &lt; lim4)) &amp;</span>
<span class="sd">               ((col1+3.1*col2+col3*col4) &gt; lim5))&#39;&#39;&#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>it is possible to determine, by analysing the data distribution and the query
limits, that col4 is such a <em>main column</em>.  So, by ordering the table by the
col4 column (for example, by specifying setting the column to sort by in the
sortby parameter in the <a class="reference internal" href="libref/structured_storage.html#tables.Table.copy" title="tables.Table.copy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Table.copy()</span></code></a> method and re-indexing col2 and
col4 afterwards, we should get much faster performance for our query.  This
is effectively demonstrated in <a class="reference internal" href="#querytimes-indexed-sorted"><span class="std std-ref">Figure 12</span></a>,
where one can see how queries with a low to medium (up to 10000) number of
hits can be done in around 1 tenth of second for a RAID-0 setup and in around
1 hundredth of second for a SSD disk.  This represents up to more that 100x
improvement in speed with respect to the times with unsorted tables.  On the
other hand, when the number of hits is large (&gt; 1 million), the query times
grow almost linearly, showing a near-perfect scalability for both RAID-0 and
SSD setups (the sequential access to disk becomes the bottleneck in this
case).</p>
<figure class="align-center" id="id23">
<span id="querytimes-indexed-sorted"></span><img alt="../_images/Q8-1g-idx-sorted.png" src="../_images/Q8-1g-idx-sorted.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 12. Times for complex queries with a cold cache (mean of 5 first
random queries) for unsorted and sorted tables.</strong></span><a class="headerlink" href="#id23" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Even though we have shown many ways to improve query times that should
fulfill the needs of most of people, for those needing more, you can for sure
discover new optimization opportunities.  For example, querying against
sorted tables is limited mainly by sequential access to data on disk and data
compression capability, so you may want to read <a class="reference internal" href="#chunksizefinetune"><span class="std std-ref">Fine-tuning the chunksize</span></a>, for
ways on improving this aspect.
Reading the other sections of this chapter will help in finding new roads for
increasing the performance as well.  You know, the limit for stopping the
optimization process is basically your imagination (but, most plausibly, your
available time ;-).</p>
</section>
</section>
<section id="compression-issues">
<span id="compressionissues"></span><h2>Compression issues<a class="headerlink" href="#compression-issues" title="Link to this heading"></a></h2>
<p>One of the beauties of PyTables is that it supports compression on tables and
arrays <a class="footnote-reference brackets" href="#id5" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, although it is not used by default. Compression of big amounts
of data might be a bit controversial feature, because it has a legend of
being a very big consumer of CPU time resources. However, if you are willing
to check if compression can help not only by reducing your dataset file size
but <em>also</em> by improving I/O efficiency, specially when dealing with very
large datasets, keep reading.</p>
<section id="a-study-on-supported-compression-libraries">
<h3>A study on supported compression libraries<a class="headerlink" href="#a-study-on-supported-compression-libraries" title="Link to this heading"></a></h3>
<p>The compression library used by default is the <em>Zlib</em> (see
<a class="reference internal" href="bibliography.html#zlib"><span class="std std-ref">[ZLIB]</span></a>). Since HDF5 <em>requires</em> it, you can safely use it and
expect that your HDF5 files will be readable on any other platform that has
HDF5 libraries installed. Zlib provides good compression ratio, although
somewhat slow, and reasonably fast decompression.  Because of that, it is a
good candidate to be used for compressing you data.</p>
<p>However, in some situations it is critical to have a <em>very good decompression
speed</em> (at the expense of lower compression ratios or more CPU wasted on
compression, as we will see soon). In others, the emphasis is put in
achieving the <em>maximum compression ratios</em>, no matter which reading speed
will result. This is why support for two additional compressors has been
added to PyTables: LZO (see <a class="reference internal" href="bibliography.html#lzo"><span class="std std-ref">[LZO]</span></a>) and bzip2 (see
<a class="reference internal" href="bibliography.html#bzip2"><span class="std std-ref">[BZIP2]</span></a>). Following the author of LZO (and checked by the
author of this section, as you will see soon), LZO offers pretty fast
compression and extremely fast decompression. In fact, LZO is so fast when
compressing/decompressing that it may well happen (that depends on your data,
of course) that writing or reading a compressed dataset is sometimes faster
than if it is not compressed at all (specially when dealing with extremely
large datasets). This fact is very important, specially if you have to deal
with very large amounts of data. Regarding bzip2, it has a reputation of
achieving excellent compression ratios, but at the price of spending much
more CPU time, which results in very low compression/decompression speeds.</p>
<p>Be aware that the LZO and bzip2 support in PyTables is not standard on HDF5,
so if you are going to use your PyTables files in other contexts different
from PyTables you will not be able to read them. Still, see the
<a class="reference internal" href="utilities.html#ptrepackdescr"><span class="std std-ref">ptrepack</span></a> (where the ptrepack utility is described) to find a way
to free your files from LZO or bzip2 dependencies, so that you can use these
compressors locally with the warranty that you can replace them with Zlib (or
even remove compression completely) if you want to use these files with other
HDF5 tools or platforms afterwards.</p>
<p>In order to allow you to grasp what amount of compression can be achieved,
and how this affects performance, a series of experiments has been carried
out. All the results presented in this section (and in the next one) have
been obtained with synthetic data and using PyTables 1.3. Also, the tests
have been conducted on a IBM OpenPower 720 (e-series) with a PowerPC G5 at
1.65 GHz and a hard disk spinning at 15K RPM. As your data and platform may
be totally different for your case, take this just as a guide because your
mileage may vary. Finally, and to be able to play with tables with a number
of rows as large as possible, the record size has been chosen to be small (16
bytes). Here is its definition:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Bench</span><span class="p">(</span><span class="n">IsDescription</span><span class="p">):</span>
    <span class="n">var1</span> <span class="o">=</span> <span class="n">StringCol</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">var2</span> <span class="o">=</span> <span class="n">IntCol</span><span class="p">()</span>
    <span class="n">var3</span> <span class="o">=</span> <span class="n">FloatCol</span><span class="p">()</span>
</pre></div>
</div>
<p>With this setup, you can look at the compression ratios that can be achieved
in <a class="reference internal" href="#comprtblcomparison"><span class="std std-ref">Figure 13</span></a>. As you can see, LZO is the
compressor that performs worse in this sense, but, curiously enough, there is
not much difference between Zlib and bzip2.</p>
<figure class="align-center" id="id24">
<span id="comprtblcomparison"></span><img alt="../_images/compressed-recordsize.png" src="../_images/compressed-recordsize.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 13. Comparison between different compression libraries.</strong></span><a class="headerlink" href="#id24" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Also, PyTables lets you select different compression levels for Zlib and
bzip2, although you may get a bit disappointed by the small improvement that
these compressors show when dealing with a combination of numbers and strings
as in our example. As a reference, see plot
<a class="reference internal" href="#comprzlibcomparison"><span class="std std-ref">Figure 14</span></a> for a comparison of the compression
achieved by selecting different levels of Zlib.  Very oddly, the best
compression ratio corresponds to level 1 (!).  See later for an explanation
and more figures on this subject.</p>
<figure class="align-center" id="id25">
<span id="comprzlibcomparison"></span><img alt="../_images/compressed-recordsize-zlib.png" src="../_images/compressed-recordsize-zlib.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 14. Comparison between different compression levels of Zlib.</strong></span><a class="headerlink" href="#id25" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Have also a look at <a class="reference internal" href="#comprwritecomparison"><span class="std std-ref">Figure 15</span></a>. It shows how the
speed of writing rows evolves as the size (number of rows) of the table
grows. Even though in these graphs the size of one single row is 16 bytes,
you can most probably extrapolate these figures to other row sizes.</p>
<figure class="align-center" id="id26">
<span id="comprwritecomparison"></span><img alt="../_images/compressed-writing.png" src="../_images/compressed-writing.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 15. Writing tables with several compressors.</strong></span><a class="headerlink" href="#id26" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>In <a class="reference internal" href="#comprreadnocachecomparison"><span class="std std-ref">Figure 16</span></a> you can see how compression
affects the reading performance. In fact, what you see in the plot is an
<em>in-kernel selection</em> speed, but provided that this operation is very fast
(see <a class="reference internal" href="#inkernelsearch"><span class="std std-ref">In-kernel searches</span></a>), we can accept it as an actual read test.
Compared with the reference line without compression, the general trend here
is that LZO does not affect too much the reading performance (and in some
points it is actually better), Zlib makes speed drop to a half, while bzip2
is performing very slow (up to 8x slower).</p>
<p>Also, in the same <a class="reference internal" href="#comprreadnocachecomparison"><span class="std std-ref">Figure 16</span></a> you can
notice some strange peaks in the speed that we might be tempted to attribute
to libraries on which PyTables relies (HDF5, compressors…), or to PyTables
itself.
However, <a class="reference internal" href="#comprreadcachecomparison"><span class="std std-ref">Figure 17</span></a> reveals that, if we put
the file in the filesystem cache (by reading it several times before, for
example), the evolution of the performance is much smoother. So, the most
probable explanation would be that such peaks are a consequence of the
underlying OS filesystem, rather than a flaw in PyTables (or any other
library behind it). Another consequence that can be derived from the
aforementioned plot is that LZO decompression performance is much better than
Zlib, allowing an improvement in overall speed of more than 2x, and perhaps
more important, the read performance for really large datasets (i.e. when
they do not fit in the OS filesystem cache) can be actually <em>better</em> than not
using compression at all. Finally, one can see that reading performance is
very badly affected when bzip2 is used (it is 10x slower than LZO and 4x than
Zlib), but this was somewhat expected anyway.</p>
<figure class="align-center" id="id27">
<span id="comprreadnocachecomparison"></span><img alt="../_images/compressed-select-nocache.png" src="../_images/compressed-select-nocache.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 16. Selecting values in tables with several compressors.
The file is not in the OS cache.</strong></span><a class="headerlink" href="#id27" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id28">
<span id="comprreadcachecomparison"></span><img alt="../_images/compressed-select-cache.png" src="../_images/compressed-select-cache.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 17. Selecting values in tables with several compressors.
The file is in the OS cache.</strong></span><a class="headerlink" href="#id28" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>So, generally speaking and looking at the experiments above, you can expect
that LZO will be the fastest in both compressing and decompressing, but the
one that achieves the worse compression ratio (although that may be just OK
for many situations, specially when used with shuffling - see
<a class="reference internal" href="#shufflingoptim"><span class="std std-ref">Shuffling (or how to make the compression process more effective)</span></a>).  bzip2 is the slowest, by large, in both compressing
and decompressing, and besides, it does not achieve any better compression
ratio than Zlib. Zlib represents a balance between them: it’s somewhat slow
compressing (2x) and decompressing (3x) than LZO, but it normally achieves
better compression ratios.</p>
<p>Finally, by looking at the plots <a class="reference internal" href="#comprwritezlibcomparison"><span class="std std-ref">Figure 18</span></a>,
<a class="reference internal" href="#comprreadzlibcomparison"><span class="std std-ref">Figure 19</span></a>, and the aforementioned
<a class="reference internal" href="#comprzlibcomparison"><span class="std std-ref">Figure 14</span></a> you can see why the recommended
compression level to use for all compression libraries is 1.  This is the
lowest level of compression, but as the size of the underlying HDF5 chunk
size is normally rather small compared with the size of compression buffers,
there is not much point in increasing the latter (i.e. increasing the
compression level).  Nonetheless, in some situations (like for example, in
extremely large tables or arrays, where the computed chunk size can be rather
large) you may want to check, on your own, how the different compression
levels do actually affect your application.</p>
<p>You can select the compression library and level by setting the complib and
complevel keywords in the Filters class (see <a class="reference internal" href="libref/helper_classes.html#filtersclassdescr"><span class="std std-ref">The Filters class</span></a>). A
compression level of 0 will completely disable compression (the default), 1
is the less memory and CPU time demanding level, while 9 is the maximum level
and the most memory demanding and CPU intensive. Finally, have in mind that
LZO is not accepting a compression level right now, so, when using LZO, 0
means that compression is not active, and any other value means that LZO is
active.</p>
<p>So, in conclusion, if your ultimate goal is writing and reading as fast as
possible, choose LZO. If you want to reduce as much as possible your data,
while retaining acceptable read speed, choose Zlib. Finally, if portability
is important for you, Zlib is your best bet. So, when you want to use bzip2?
Well, looking at the results, it is difficult to recommend its use in
general, but you may want to experiment with it in those cases where you know
that it is well suited for your data pattern (for example, for dealing with
repetitive string datasets).</p>
<figure class="align-center" id="id29">
<span id="comprwritezlibcomparison"></span><img alt="../_images/compressed-writing-zlib.png" src="../_images/compressed-writing-zlib.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 18. Writing in tables with different levels of compression.</strong></span><a class="headerlink" href="#id29" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id30">
<span id="comprreadzlibcomparison"></span><img alt="../_images/compressed-select-cache-zlib.png" src="../_images/compressed-select-cache-zlib.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 19. Selecting values in tables with different levels of
compression. The file is in the OS cache.</strong></span><a class="headerlink" href="#id30" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="shuffling-or-how-to-make-the-compression-process-more-effective">
<span id="shufflingoptim"></span><h3>Shuffling (or how to make the compression process more effective)<a class="headerlink" href="#shuffling-or-how-to-make-the-compression-process-more-effective" title="Link to this heading"></a></h3>
<p>The HDF5 library provides an interesting filter that can leverage the results
of your favorite compressor. Its name is <em>shuffle</em>, and because it can
greatly benefit compression and it does not take many CPU resources (see
below for a justification), it is active <em>by default</em> in PyTables whenever
compression is activated (independently of the chosen compressor). It is
deactivated when compression is off (which is the default, as you already
should know). Of course, you can deactivate it if you want, but this is not
recommended.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since PyTables 3.3, a new <em>bitshuffle</em> filter for Blosc compressor
has been added.  Contrarily to <em>shuffle</em> that shuffles bytes,
<em>bitshuffle</em> shuffles the chunk data at bit level which <strong>could</strong>
improve compression ratios at the expense of some speed penalty.
Look at the <a class="reference internal" href="libref/helper_classes.html#filtersclassdescr"><span class="std std-ref">The Filters class</span></a> documentation on how to
activate bitshuffle and experiment with it so as to decide if it
can be useful for you.</p>
</div>
<p>So, how does this mysterious filter exactly work? From the HDF5 reference
manual:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;The shuffle filter de-interlaces a block of data by reordering the</span>
<span class="nb">bytes</span><span class="o">.</span> <span class="n">All</span> <span class="n">the</span> <span class="nb">bytes</span> <span class="kn">from</span> <span class="nn">one</span> <span class="n">consistent</span> <span class="n">byte</span> <span class="n">position</span> <span class="n">of</span> <span class="n">each</span> <span class="n">data</span>
<span class="n">element</span> <span class="n">are</span> <span class="n">placed</span> <span class="n">together</span> <span class="ow">in</span> <span class="n">one</span> <span class="n">block</span><span class="p">;</span> <span class="nb">all</span> <span class="nb">bytes</span> <span class="kn">from</span> <span class="nn">a</span> <span class="n">second</span>
<span class="n">consistent</span> <span class="n">byte</span> <span class="n">position</span> <span class="n">of</span> <span class="n">each</span> <span class="n">data</span> <span class="n">element</span> <span class="n">are</span> <span class="n">placed</span> <span class="n">together</span> <span class="n">a</span>
<span class="n">second</span> <span class="n">block</span><span class="p">;</span> <span class="n">etc</span><span class="o">.</span> <span class="n">For</span> <span class="n">example</span><span class="p">,</span> <span class="n">given</span> <span class="n">three</span> <span class="n">data</span> <span class="n">elements</span> <span class="n">of</span> <span class="n">a</span> <span class="mi">4</span><span class="o">-</span><span class="n">byte</span>
<span class="n">datatype</span> <span class="n">stored</span> <span class="k">as</span> <span class="mi">012301230123</span><span class="p">,</span> <span class="n">shuffling</span> <span class="n">will</span> <span class="n">re</span><span class="o">-</span><span class="n">order</span> <span class="n">data</span> <span class="k">as</span>
<span class="mf">000111222333.</span> <span class="n">This</span> <span class="n">can</span> <span class="n">be</span> <span class="n">a</span> <span class="n">valuable</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">an</span> <span class="n">effective</span> <span class="n">compression</span>
<span class="n">algorithm</span> <span class="n">because</span> <span class="n">the</span> <span class="nb">bytes</span> <span class="ow">in</span> <span class="n">each</span> <span class="n">byte</span> <span class="n">position</span> <span class="n">are</span> <span class="n">often</span> <span class="n">closely</span>
<span class="n">related</span> <span class="n">to</span> <span class="n">each</span> <span class="n">other</span> <span class="ow">and</span> <span class="n">putting</span> <span class="n">them</span> <span class="n">together</span> <span class="n">can</span> <span class="n">increase</span> <span class="n">the</span>
<span class="n">compression</span> <span class="n">ratio</span><span class="o">.</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>In <a class="reference internal" href="#comprshufflecomparison"><span class="std std-ref">Figure 20</span></a> you can see a benchmark that
shows how the <em>shuffle</em> filter can help the different libraries in
compressing data. In this experiment, shuffle has made LZO compress almost 3x
more (!), while Zlib and bzip2 are seeing improvements of 2x. Once again, the
data for this experiment is synthetic, and <em>shuffle</em> seems to do a great work
with it, but in general, the results will vary in each case <a class="footnote-reference brackets" href="#id6" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
<figure class="align-center" id="id31">
<span id="comprshufflecomparison"></span><img alt="../_images/compressed-recordsize-shuffle.png" src="../_images/compressed-recordsize-shuffle.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 20. Comparison between different compression libraries with and
without the shuffle filter.</strong></span><a class="headerlink" href="#id31" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>At any rate, the most remarkable fact about the <em>shuffle</em> filter is the
relatively high level of compression that compressor filters can achieve when
used in combination with it. A curious thing to note is that the Bzip2
compression rate does not seem very much improved (less than a 40%), and what
is more striking, Bzip2+shuffle does compress quite <em>less</em> than Zlib+shuffle
or LZO+shuffle combinations, which is kind of unexpected. The thing that
seems clear is that Bzip2 is not very good at compressing patterns that
result of shuffle application. As always, you may want to experiment with
your own data before widely applying the Bzip2+shuffle combination in order
to avoid surprises.</p>
<p>Now, how does shuffling affect performance? Well, if you look at plots
<a class="reference internal" href="#comprwriteshufflecomparison"><span class="std std-ref">Figure 21</span></a>,
<a class="reference internal" href="#comprreadnocacheshufflecomparison"><span class="std std-ref">Figure 22</span></a> and
<a class="reference internal" href="#comprreadcacheshufflecomparison"><span class="std std-ref">Figure 23</span></a>, you will get a somewhat
unexpected (but pleasant) surprise. Roughly, <em>shuffle</em> makes the writing
process (shuffling+compressing) faster (approximately a 15% for LZO, 30% for
Bzip2 and a 80% for Zlib), which is an interesting result by itself.
But perhaps more exciting is the fact that the reading process
(unshuffling+decompressing) is also accelerated by a similar extent (a 20%
for LZO, 60% for Zlib and a 75% for Bzip2, roughly).</p>
<figure class="align-center" id="id32">
<span id="comprwriteshufflecomparison"></span><img alt="../_images/compressed-writing-shuffle.png" src="../_images/compressed-writing-shuffle.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 21. Writing with different compression libraries with and
without the shuffle filter.</strong></span><a class="headerlink" href="#id32" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id33">
<span id="comprreadnocacheshufflecomparison"></span><img alt="../_images/compressed-select-nocache-shuffle-only.png" src="../_images/compressed-select-nocache-shuffle-only.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 22. Reading with different compression libraries with the
shuffle filter. The file is not in OS cache.</strong></span><a class="headerlink" href="#id33" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id34">
<span id="comprreadcacheshufflecomparison"></span><img alt="../_images/compressed-select-cache-shuffle.png" src="../_images/compressed-select-cache-shuffle.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 23. Reading with different compression libraries with and
without the shuffle filter. The file is in OS cache.</strong></span><a class="headerlink" href="#id34" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>You may wonder why introducing another filter in the write/read pipelines
does effectively accelerate the throughput. Well, maybe data elements are
more similar or related column-wise than row-wise, i.e. contiguous elements
in the same column are more alike, so shuffling makes the job of the
compressor easier (faster) and more effective (greater ratios). As a side
effect, compressed chunks do fit better in the CPU cache (at least, the
chunks are smaller!) so that the process of unshuffle/decompress can make a
better use of the cache (i.e. reducing the number of CPU cache faults).</p>
<p>So, given the potential gains (faster writing and reading, but specially
much improved compression level), it is a good thing to have such a filter
enabled by default in the battle for discovering redundancy when you want to
compress your data, just as PyTables does.</p>
</section>
<section id="avoiding-filter-pipeline-overhead-with-direct-chunking">
<span id="comprdirectchunking"></span><h3>Avoiding filter pipeline overhead with direct chunking<a class="headerlink" href="#avoiding-filter-pipeline-overhead-with-direct-chunking" title="Link to this heading"></a></h3>
<p>Even if you hit a sweet performance spot in your combination of chunk size,
compression and shuffling as discussed in the previous section, that may still
not be enough for particular scenarios demanding very high I/O speeds (e.g. to
cope with continuous collection or extraction of data).  As shown in the
section about <a class="reference internal" href="#multidimslicing"><span class="std std-ref">multidimensional slicing</span></a>, the HDF5
filter pipeline still introduces a significant overhead in the processing of
chunk data for storage.  Here is where <a class="reference internal" href="#directchunking"><span class="std std-ref">direct chunking</span></a>
may help you squeeze that needed extra performance.</p>
<p>The notebook bench/direct-chunking-AMD-7800X3D.ipynb shows an experiment run
on an AMD Ryzen 7 7800X3D CPU with 8 cores, 96 MB L3 cache and 8 MB L2 cache,
clocked at 4.2 GHz.  The scenario is similar to that of
examples/direct-chunking.py, with a tomography-like dataset (consisting of a
stack of 10 greyscale images) stored as an EArray, where each image
corresponds to a chunk:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25600</span><span class="p">,</span> <span class="mi">19200</span><span class="p">)</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;u2&#39;</span><span class="p">)</span>
<span class="n">chunkshape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>
</div>
<p>Chunks are compressed with Zstd at level 5 via Blosc2, both for direct and
regular chunking.  Regular operations use plain slicing to write and read each
image/chunk:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">array</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">np_data</span>

<span class="c1"># Read</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">np_data2</span> <span class="o">=</span> <span class="n">array</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
</pre></div>
</div>
<p>In contrast, direct operations need to manually perform the (de)compression,
(de)serialization and writing/reading of each image/chunk:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write</span>
<span class="n">coords_tail</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">b2_data</span> <span class="o">=</span> <span class="n">b2</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">np_data</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="n">chunkshape</span><span class="p">,</span>
                         <span class="n">cparams</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">clevel</span><span class="o">=</span><span class="n">clevel</span><span class="p">))</span>
    <span class="n">wchunk</span> <span class="o">=</span> <span class="n">b2_data</span><span class="o">.</span><span class="n">to_cframe</span><span class="p">()</span>
    <span class="n">chunk_coords</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span><span class="p">,)</span> <span class="o">+</span> <span class="n">coords_tail</span>
    <span class="n">array</span><span class="o">.</span><span class="n">write_chunk</span><span class="p">(</span><span class="n">chunk_coords</span><span class="p">,</span> <span class="n">wchunk</span><span class="p">)</span>

<span class="c1"># Read</span>
<span class="n">coords_tail</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">chunk_coords</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span><span class="p">,)</span> <span class="o">+</span> <span class="n">coords_tail</span>
    <span class="n">rchunk</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">read_chunk</span><span class="p">(</span><span class="n">chunk_coords</span><span class="p">)</span>
    <span class="n">ndarr</span> <span class="o">=</span> <span class="n">b2</span><span class="o">.</span><span class="n">ndarray_from_cframe</span><span class="p">(</span><span class="n">rchunk</span><span class="p">)</span>
    <span class="n">np_data2</span> <span class="o">=</span> <span class="n">ndarr</span><span class="p">[:]</span>
</pre></div>
</div>
<p>Despite the more elaborate handling of data, we rest assured that HDF5
performs no additional processing of chunks, while regular chunking implies
data going through the whole filter pipeline.  Plotting the results shows that
direct chunking yields 3.75x write and 4.4x read speedups, reaching write/read
speeds of 1.7 GB/s and 5.2 GB/s.</p>
<figure class="align-center" id="id35">
<img alt="../_images/direct-chunking-AMD-7800X3D.png" src="../_images/direct-chunking-AMD-7800X3D.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 24. Comparing write/read speeds of regular vs. direct chunking
(AMD 7800X3D).</strong></span><a class="headerlink" href="#id35" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Write performance may benefit even more of higher core counts, as shown in the
plot below, where the same benchmark on an Intel Core i9-13900K CPU (24 mixed
cores, 32 MB L2, 5.7 GHz) raises write speedup to 4.6x (2.6 GB/s).</p>
<figure class="align-center" id="id36">
<img alt="../_images/direct-chunking-i13900K.png" src="../_images/direct-chunking-i13900K.png" />
<figcaption>
<p><span class="caption-text"><strong>Figure 25. Comparing write/read speeds of regular vs. direct chunking
(Intel 13900K).</strong></span><a class="headerlink" href="#id36" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>As we can see, bypassing the HDF5 pipeline with direct chunking offers
immediate speed benefits for both writing and reading.  This is particularly
beneficial for large datasets, where the overhead of the pipeline can be very
significant.</p>
<p>By using the Blosc2 library to serialize the data with the direct chunking
method, the resulting HDF5 file can still be decompressed with any
HDF5-enabled application, as long as it has the Blosc2 filter available
(e.g. via hdf5plugin).  Direct chunking allows for more direct interaction
with the Blosc2 library, so you can experiment with different blockshapes,
compressors, filters and compression levels, to find the best configuration
for your specific needs.</p>
</section>
</section>
<section id="getting-the-most-from-the-node-lru-cache">
<span id="lruoptim"></span><h2>Getting the most from the node LRU cache<a class="headerlink" href="#getting-the-most-from-the-node-lru-cache" title="Link to this heading"></a></h2>
<p>One limitation of the initial versions of PyTables was that they needed to
load all nodes in a file completely before being ready to deal with them,
making the opening times for files with a lot of nodes very high and
unacceptable in many cases.</p>
<p>Starting from PyTables 1.2 on, a new lazy node loading schema was setup that
avoids loading all the nodes of the <em>object tree</em> in memory. In addition, a
new LRU cache was introduced in order to accelerate the access to already
visited nodes. This cache (one per file) is responsible for keeping up the
most recently visited nodes in memory and discard the least recent used ones.
This represents a big advantage over the old schema, not only in terms of
memory usage (as there is no need to load <em>every</em> node in memory), but it
also adds very convenient optimizations for working interactively like, for
example, speeding-up the opening times of files with lots of nodes, allowing
to open almost any kind of file in typically less than one tenth of second
(compare this with the more than 10 seconds for files with more than 10000
nodes in PyTables pre-1.2 era) as well as optimizing the access to frequently
visited nodes. See for more info on the advantages (and also drawbacks) of
this approach.</p>
<p>One thing that deserves some discussion is the election of the parameter that
sets the maximum amount of nodes to be kept in memory at any time.
As PyTables is meant to be deployed in machines that can have potentially low
memory, the default for it is quite conservative (you can look at its actual
value in the <a class="reference internal" href="parameter_files.html#tables.parameters.NODE_CACHE_SLOTS" title="tables.parameters.NODE_CACHE_SLOTS"><code class="xref py py-data docutils literal notranslate"><span class="pre">parameters.NODE_CACHE_SLOTS</span></code></a> parameter in module
<code class="file docutils literal notranslate"><span class="pre">tables/parameters.py</span></code>). However, if you usually need to deal with
files that have many more nodes than the maximum default, and you have a lot
of free memory in your system, then you may want to experiment in order to
see which is the appropriate value of <a class="reference internal" href="parameter_files.html#tables.parameters.NODE_CACHE_SLOTS" title="tables.parameters.NODE_CACHE_SLOTS"><code class="xref py py-data docutils literal notranslate"><span class="pre">parameters.NODE_CACHE_SLOTS</span></code></a> that
fits better your needs.</p>
<p>As an example, look at the next code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">browse_tables</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">fileh</span> <span class="o">=</span> <span class="n">open_file</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">fileh</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">newgroup</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">tt</span> <span class="ow">in</span> <span class="n">fileh</span><span class="o">.</span><span class="n">walk_nodes</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s2">&quot;Table&quot;</span><span class="p">):</span>
            <span class="n">title</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">attrs</span><span class="o">.</span><span class="n">TITLE</span>
            <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">tt</span><span class="p">:</span>
                <span class="k">pass</span>
    <span class="n">fileh</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
<p>We will be running the code above against a couple of files having a
<code class="docutils literal notranslate"><span class="pre">/newgroup</span></code> containing 100 tables and 1000 tables respectively.  In addition,
this benchmark is run twice for two different values of the LRU cache size,
specifically 256 and 1024. You can see the results in
<a class="reference internal" href="#optimization-table-1"><span class="std std-ref">table</span></a>.</p>
<span id="optimization-table-1"></span><table class="docutils align-default" id="id37">
<caption><span class="caption-text"><strong>Retrieval speed and memory consumption depending on the number of nodes in LRU cache.</strong></span><a class="headerlink" href="#id37" title="Link to this table"></a></caption>
<thead>
<tr class="row-odd"><th class="head" colspan="2"><p>Number:</p></th>
<th class="head" colspan="4"><p>100 nodes</p></th>
<th class="head" colspan="4"><p>1000 nodes</p></th>
</tr>
<tr class="row-even"><th class="head" colspan="2"><p>Mem &amp; Speed</p></th>
<th class="head" colspan="2"><p>Memory (MB)</p></th>
<th class="head" colspan="2"><p>Time (ms)</p></th>
<th class="head" colspan="2"><p>Memory (MB)</p></th>
<th class="head" colspan="2"><p>Time (ms)</p></th>
</tr>
<tr class="row-odd"><th class="head"><p>Node is coming from…</p></th>
<th class="head"><p>Cache size</p></th>
<th class="head"><p>256</p></th>
<th class="head"><p>1024</p></th>
<th class="head"><p>256</p></th>
<th class="head"><p>1024</p></th>
<th class="head"><p>256</p></th>
<th class="head"><p>1024</p></th>
<th class="head"><p>256</p></th>
<th class="head"><p>1024</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Disk</p></td>
<td></td>
<td><p>14</p></td>
<td><p>14</p></td>
<td><p>1.24</p></td>
<td><p>1.24</p></td>
<td><p>51</p></td>
<td><p>66</p></td>
<td><p>1.33</p></td>
<td><p>1.31</p></td>
</tr>
<tr class="row-odd"><td><p>Cache</p></td>
<td></td>
<td><p>14</p></td>
<td><p>14</p></td>
<td><p>0.53</p></td>
<td><p>0.52</p></td>
<td><p>65</p></td>
<td><p>73</p></td>
<td><p>1.35</p></td>
<td><p>0.68</p></td>
</tr>
</tbody>
</table>
<p>From the data in <a class="reference internal" href="#optimization-table-1"><span class="std std-ref">table</span></a>, one can see that when
the number of objects that you are dealing with does fit in cache, you will
get better access times to them. Also, incrementing the node cache size
effectively consumes more memory <em>only</em> if the total nodes exceeds the slots
in cache; otherwise the memory consumption remains the same. It is also worth
noting that incrementing the node cache size in the case you want to fit all
your nodes in cache does not take much more memory than being too
conservative. On the other hand, it might happen that the speed-up that you
can achieve by allocating more slots in your cache is not worth the amount of
memory used.</p>
<p>Also worth noting is that if you have a lot of memory available and
performance is absolutely critical, you may want to try out a negative value
for <a class="reference internal" href="parameter_files.html#tables.parameters.NODE_CACHE_SLOTS" title="tables.parameters.NODE_CACHE_SLOTS"><code class="xref py py-data docutils literal notranslate"><span class="pre">parameters.NODE_CACHE_SLOTS</span></code></a>.  This will cause that all the touched
nodes will be kept in an internal dictionary and this is the faster way to
load/retrieve nodes.
However, and in order to avoid a large memory consumption, the user will be
warned when the number of loaded nodes will reach the <code class="docutils literal notranslate"><span class="pre">-NODE_CACHE_SLOTS</span></code>
value.</p>
<p>Finally, a value of zero in <a class="reference internal" href="parameter_files.html#tables.parameters.NODE_CACHE_SLOTS" title="tables.parameters.NODE_CACHE_SLOTS"><code class="xref py py-data docutils literal notranslate"><span class="pre">parameters.NODE_CACHE_SLOTS</span></code></a> means that
any cache mechanism is disabled.</p>
<p>At any rate, if you feel that this issue is important for you, there is no
replacement for setting your own experiments up in order to proceed to
fine-tune the <a class="reference internal" href="parameter_files.html#tables.parameters.NODE_CACHE_SLOTS" title="tables.parameters.NODE_CACHE_SLOTS"><code class="xref py py-data docutils literal notranslate"><span class="pre">parameters.NODE_CACHE_SLOTS</span></code></a> parameter.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>PyTables &gt;= 2.3 sports an optimized LRU cache node written in C, so
you should expect significantly faster LRU cache operations when
working with it.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Numerical results reported in <a class="reference internal" href="#optimization-table-1"><span class="std std-ref">table</span></a> have been
obtained with PyTables &lt; 3.1. In PyTables 3.1 the node cache mechanism has
been completely redesigned so while all comments above are still valid,
numerical values could be a little bit different from the ones reported in
<a class="reference internal" href="#optimization-table-1"><span class="std std-ref">table</span></a>.</p>
</div>
</section>
<section id="compacting-your-pytables-files">
<h2>Compacting your PyTables files<a class="headerlink" href="#compacting-your-pytables-files" title="Link to this heading"></a></h2>
<p>Let’s suppose that you have a file where you have made a lot of row deletions
on one or more tables, or deleted many leaves or even entire subtrees. These
operations might leave <em>holes</em> (i.e. space that is not used anymore) in your
files that may potentially affect not only the size of the files but, more
importantly, the performance of I/O. This is because when you delete a lot of
rows in a table, the space is not automatically recovered on the fly.
In addition, if you add many more rows to a table than specified in the
expectedrows keyword at creation time this may affect performance as well, as
explained in <a class="reference internal" href="#expectedrowsoptim"><span class="std std-ref">Informing PyTables about expected number of rows in tables or arrays</span></a>.</p>
<p>In order to cope with these issues, you should be aware that PyTables
includes a handy utility called ptrepack which can be very useful not only to
compact <em>fragmented</em> files, but also to adjust some internal parameters in
order to use better buffer and chunk sizes for optimum I/O speed.
Please check the <a class="reference internal" href="utilities.html#ptrepackdescr"><span class="std std-ref">ptrepack</span></a> for a brief tutorial on its use.</p>
<p>Another thing that you might want to use ptrepack for is changing the
compression filters or compression levels on your existing data for different
goals, like checking how this can affect both final size and I/O performance,
or getting rid of the optional compressors like LZO or bzip2 in your existing
files, in case you want to use them with generic HDF5 tools that do not have
support for these filters.</p>
<hr class="docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id4" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>CArray nodes, though not
extensible, are chunked and have their optimum chunk size
automatically computed at creation time, since their final shape is known.</p>
</aside>
<aside class="footnote brackets" id="id5" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Except for Array objects.</p>
</aside>
<aside class="footnote brackets" id="id6" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Some users reported that the typical improvement with real
data is between a factor 1.5x and 2.5x over the already compressed
datasets.</p>
</aside>
</aside>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="libref/filenode_classes.html" class="btn btn-neutral float-left" title="Filenode Module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="filenode.html" class="btn btn-neutral float-right" title="filenode - simulating a filesystem with PyTables" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2011–2025, PyTables maintainers.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>